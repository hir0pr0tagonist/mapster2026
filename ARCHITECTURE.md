# Mapster Cloud Solution Architecture

## Overview
This document describes the architecture of the Mapster Cloud solution, a containerized stack for serving an interactive MapLibre map backed by PostGIS.

The system supports two ways of delivering administrative boundaries to the browser:

- **Vector tiles (MVT)** for fast, incremental loading as the user pans/zooms.
- **GeoJSON overlays** for debugging, inspection, and simple integration.

## Components

### 1. postgis (Database Service)
- **Type:** Containerized PostgreSQL with PostGIS extension
- **Purpose:** Stores all geospatial data (boundaries, polygons, etc.) and supports spatial queries and vector tile generation.
- **Persistence:** Uses a Docker volume for data durability.
- **Port:** 5432 (internal)

**Key table**

- `admin_areas` (SRID 4326): a denormalized table holding admin boundaries across multiple hierarchy depths.
  - Hierarchy depth is inferred by NULL/NOT NULL checks of `name_1..name_5`.

**Facts + metrics (non-geometric domain data)**

To support shading admin areas by one-dimensional metrics (e.g. average land price per m²) while keeping request latency low, Mapster uses a Raw → Canonical → Aggregates pattern in Postgres:

- `facts_raw.raw_record`: append-only landing zone for diverse incoming data formats (CSV, scrapes, exports).
- `facts.observation`: normalized observations with optional point geometry + assigned admin-area key.
- `facts_agg.area_metric_daily`: pre-aggregated daily rollups used by the API for fast reads.

Administrative matching uses a **stable area key** derived from `gid_0..gid_5` (not the surrogate `id`, which changes when the import job recreates `admin_areas`).

For fast “aggregate up” behavior (Option 1: never distribute down), the import job rebuilds a closure table:

- `geo.admin_area_ancestors`: `(area_key → ancestor_key)` mappings (including self) with distance.

### 2. api (Spring Boot Backend Service)
- **Type:** Java 17+, Spring Boot, REST API
- **Purpose:**
  - Serves GeoJSON overlays for debugging and inspection.
  - Serves Mapbox Vector Tiles (MVT) for fast map rendering.
  - Applies performance optimizations: bbox prefiltering, simplified geometries (when applicable), gzip compression, and HTTP caching headers.
- **Port:** 8080 (internal/external)
- **Build:** Multi-stage Dockerfile (Maven build, Temurin JDK runtime)

**Key endpoints**

- `GET /api/tiles/{z}/{x}/{y}.mvt`
  - Returns `application/vnd.mapbox-vector-tile` generated by PostGIS (`ST_TileEnvelope`, `ST_AsMVTGeom`, `ST_AsMVT`).
  - Uses zoom-to-depth mapping (OSM zoom 6..11 → depth 0..5) implemented via `name_1..name_5` nullability.
  - Adds `Cache-Control` and a lightweight weak `ETag` so clients can cache aggressively and get fast `304 Not Modified` responses.
- `GET /api/overlays`
  - Returns a GeoJSON `FeatureCollection` for a supplied bbox.
  - Uses bbox prefiltering (`geom && envelope`) plus exact intersects, and emits valid GeoJSON Features.
  - Applies simplification in meters (EPSG:3857) + snap-to-grid, transformed back to 4326, to reduce payload while keeping borders visually consistent.

**Metrics endpoints**

- `GET /api/area-metrics`
  - Returns admin-area GeoJSON for a bbox, enriched with metric rollups from `facts_agg.area_metric_daily`.
  - Parameters: `minLon`, `minLat`, `maxLon`, `maxLat`, `metricId`, optional `depth`/`zoom`, optional `from`/`to` (defaults to last 30 days).

**Ingestion endpoints (initial scaffold)**

- `POST /api/ingest/raw`
  - Stores an arbitrary JSON payload into `facts_raw.raw_record` (append-only), for later normalization.
- `POST /api/ingest/observation`
  - Accepts a canonical observation (metric/value/time) with either coordinates or an already-known `assignedAreaKey`.
  - If coordinates are provided, the API assigns the most detailed containing admin area via point-in-polygon.
  - Updates daily rollups for the assigned area and all its ancestors.

**Synthetic demo data (clustered)**

For demos and development, the database provides a generator that creates spatially clustered observations (e.g. expensive areas tend to be near each other) and updates rollups in one call:

- `SELECT * FROM facts.generate_synthetic_clustered_observations(...)`

This inserts into `facts.observation` and updates `facts_agg.area_metric_daily` via `geo.admin_area_ancestors` (Option 1 semantics: only aggregates upward).

**Routing note (Ingress-friendly)**

When running behind Kubernetes Ingress under a single host, the API is served under `/api/...` via `server.servlet.context-path=/api`.
Controller mappings intentionally do not hardcode `/api` (e.g. `/tiles/...`, `/overlays`) so the same code works both behind Ingress and when port-forwarded.

**HTTP performance**

- **gzip** is enabled for large responses, including MVT.
- **ETag/304** is implemented for both tiles and overlays.
- **Cache-Control** is set to allow client/proxy caching (tiles are relatively static).

### 3. web (Frontend Service)
- **Type:** Nginx static file server
- **Purpose:**
  - Serves static frontend assets (HTML, JS, CSS) using Nginx.
  - Renders the OSM basemap and overlays using MapLibre GL JS.
  - Uses a vector tile source pointed at the backend MVT endpoint to render boundaries efficiently.
- **Port:** 8081 (external)

**Rendering approach (MapLibre)**

- Raster OSM basemap.
- Vector tile source for admin boundaries.
- Layer stack:
  - Transparent fill layer for hit-testing.
  - Hover highlight fill placed under boundary lines.
  - Boundary line layer on top.

### 4. import (Data Import Service)
- **Type:** Custom GDAL/ogr2ogr container
- **Purpose:**
  - Automates loading of large GeoPackage or other geospatial data into PostGIS at startup.
  - Waits for PostGIS readiness before import.

**Import behavior**

- Imports administrative boundary layers from a GeoPackage (GPKG) into `admin_areas`.
- Drops/refreshes data to avoid duplication across runs.
- Imports layers deterministically to avoid repeated ADM_0 rows.

After importing `admin_areas`, the import script also rebuilds `geo.admin_area_ancestors` and creates `geo.admin_areas` (a convenience view with computed `area_key` and `depth`).

**Kubernetes note (volume shadowing)**

In Kubernetes, it's common to mount a volume at `/data`. A volume mount hides any files baked into the
container image at the same path. To avoid the import script "disappearing" when `/data` is mounted,
the import image places the script at `/usr/local/bin/upload_geopackage.sh` and executes it from there.

**Kubernetes import (cloud-ready)**

For managed clusters, the import Job uses an initContainer to download the GPKG from S3-compatible object storage into an `emptyDir` volume before running the import container. Credentials and download parameters are provided via a Kubernetes Secret (see the STACKIT overlay documentation).

## Networking
- All services are on the default Docker Compose network, allowing inter-service communication by container name (e.g., `api`, `postgis`).
- Frontend (web) calls backend (api) via HTTP. In local dev, the browser uses host ports:
  - `http://localhost:8081` for the UI
  - `http://localhost:8080` for API endpoints

### Kubernetes (managed cluster) networking

In Kubernetes deployments (e.g. STACKIT), `web` and `api` are exposed via an Ingress on a single host:

- `/` routes to `web`
- `/api` routes to `api`

Ingress is handled by an Ingress controller (e.g. ingress-nginx). The Ingress resource must specify an IngressClass (for ingress-nginx: `spec.ingressClassName: nginx`) so the controller actually reconciles it.

## Data Flow
1. **User loads map in browser (localhost:8081).**
2. **MapLibre requests vector tiles from backend API (`/api/tiles/{z}/{x}/{y}.mvt`) as the user pans/zooms.**
3. **API runs an MVT query in PostGIS constrained to the tile envelope (fast index-friendly filtering + clipping).**
4. **API returns MVT bytes with gzip + caching headers.**
5. **Frontend renders boundaries as vector line/fill layers and supports hover highlighting.**

GeoJSON overlays remain available for debugging and diagnostics (`/api/overlays`).

## Scaling & Best Practices
- **Stateless services:** api and web containers are stateless and can be scaled horizontally.
- **Separation of concerns:** Each service has a single responsibility (DB, API, frontend, import).
- **Cloud-native:** All components are containerized and orchestrated via Docker Compose.
- **Extensible:** Easy to add new REST endpoints, frontend features, or data sources.

## Notes on Zoom/Depth
Administrative hierarchy is stored in a single table; “depth” is inferred via name nullability:

- depth 0: `name_1..name_5` are NULL
- depth 1: `name_1` NOT NULL, `name_2..name_5` NULL
- …
- depth 5: `name_1..name_5` NOT NULL

For both overlays and tiles, OSM zoom 6..11 is mapped to depth 0..5.

## File/Directory Structure (Key Parts)
- `docker-compose.yml` — Orchestrates all services.
- `api/` — Spring Boot backend (Dockerfile, pom.xml, source code).
- `web/` — Frontend (index.html, JS, Nginx Dockerfile).
- `postgis/initdb/` — PostGIS initialization scripts.
- `postgis/import/` — Data import scripts (GeoPackage → PostGIS).
- `k8s/` — Kubernetes manifests (minikube-ready) for PostGIS, API, web, ingress, and the import job.

### STACKIT overlay

For managed Kubernetes (STACKIT), the repo includes a Kustomize overlay that:

- uses versioned images from a real registry
- configures image pull secrets for private registries
- replaces the local hostPath import flow with an object-storage download
- pins IngressClass and real hostnames

See [k8s/overlays/stackit/](k8s/overlays/stackit/).

### PostGIS on Kubernetes volumes

On some storage backends, mounting the PV directly at `/var/lib/postgresql/data` causes `initdb` to fail because the mount contains a `lost+found` directory. The Kubernetes manifests set `PGDATA` to a subdirectory (e.g. `/var/lib/postgresql/data/pgdata`) to avoid this.

## Future Extensions
- Add authentication/authorization to API.
- Consider longer-lived tile caching + immutable URLs for static datasets.
- Add monitoring (request timing, tile sizes, DB query latency).
- Add optional generalization per zoom for even smaller tiles.

---
This manifest is a living document. Update as the architecture evolves.
